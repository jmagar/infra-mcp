"""
ZFS Protection Service

Specialized protection logic for ZFS pool and dataset operations. Provides
advanced safety mechanisms for ZFS operations including snapshot validation,
dependency checking, and data preservation strategies.
"""

import logging
from datetime import datetime, timezone, timedelta
from enum import Enum
from typing import Any

from .patterns import DestructiveActionType

logger = logging.getLogger(__name__)


class ZFSOperationType(Enum):
    """Types of ZFS operations requiring protection."""

    POOL_DESTROY = "pool_destroy"
    POOL_EXPORT = "pool_export"
    POOL_OFFLINE = "pool_offline"
    DATASET_DESTROY = "dataset_destroy"
    DATASET_RENAME = "dataset_rename"
    SNAPSHOT_DESTROY = "snapshot_destroy"
    SNAPSHOT_ROLLBACK = "snapshot_rollback"
    VOLUME_DESTROY = "volume_destroy"
    PROPERTY_RESET = "property_reset"


class ZFSProtectionLevel(Enum):
    """ZFS-specific protection levels."""

    MINIMAL = "minimal"  # Basic confirmation required
    STANDARD = "standard"  # Confirmation + snapshot validation
    ENHANCED = "enhanced"  # + dependency checking + backup validation
    MAXIMUM = "maximum"  # + manual review + multi-admin approval


class ZFSDataClassification(Enum):
    """Data classification for ZFS datasets."""

    SYSTEM_CRITICAL = "system_critical"  # OS, boot, root datasets
    APPLICATION_DATA = "application_data"  # Database, application storage
    USER_DATA = "user_data"  # Home directories, documents
    BACKUP_DATA = "backup_data"  # Backup repositories
    TEMPORARY = "temporary"  # Cache, temp data
    ARCHIVE = "archive"  # Long-term archive storage


class ZFSProtectionService:
    """
    Specialized protection service for ZFS filesystem operations.

    Features:
    - ZFS-specific risk assessment and classification
    - Automatic snapshot creation before destructive operations
    - Dataset dependency analysis and protection
    - Data classification-based protection policies
    - ZFS property validation and rollback protection
    - Integration with backup validation systems
    """

    def __init__(self):
        """Initialize the ZFS protection service."""

        # Protection policies by data classification
        self.protection_policies = {
            ZFSDataClassification.SYSTEM_CRITICAL: ZFSProtectionLevel.MAXIMUM,
            ZFSDataClassification.APPLICATION_DATA: ZFSProtectionLevel.ENHANCED,
            ZFSDataClassification.USER_DATA: ZFSProtectionLevel.ENHANCED,
            ZFSDataClassification.BACKUP_DATA: ZFSProtectionLevel.STANDARD,
            ZFSDataClassification.TEMPORARY: ZFSProtectionLevel.MINIMAL,
            ZFSDataClassification.ARCHIVE: ZFSProtectionLevel.ENHANCED,
        }

        # Operations requiring different protection levels
        self.operation_base_protection = {
            ZFSOperationType.POOL_DESTROY: ZFSProtectionLevel.MAXIMUM,
            ZFSOperationType.POOL_EXPORT: ZFSProtectionLevel.STANDARD,
            ZFSOperationType.POOL_OFFLINE: ZFSProtectionLevel.STANDARD,
            ZFSOperationType.DATASET_DESTROY: ZFSProtectionLevel.ENHANCED,
            ZFSOperationType.DATASET_RENAME: ZFSProtectionLevel.STANDARD,
            ZFSOperationType.SNAPSHOT_DESTROY: ZFSProtectionLevel.STANDARD,
            ZFSOperationType.SNAPSHOT_ROLLBACK: ZFSProtectionLevel.ENHANCED,
            ZFSOperationType.VOLUME_DESTROY: ZFSProtectionLevel.ENHANCED,
            ZFSOperationType.PROPERTY_RESET: ZFSProtectionLevel.MINIMAL,
        }

        # Critical ZFS properties that require enhanced protection
        self.critical_properties = {
            "mountpoint",
            "sharenfs",
            "sharesmb",
            "shareiscsi",
            "readonly",
            "canmount",
            "compression",
            "dedup",
            "recordsize",
            "volsize",
            "volblocksize",
        }

        logger.info("ZFSProtectionService initialized")

    async def assess_zfs_operation_risk(
        self,
        operation_type: ZFSOperationType,
        target_path: str,
        zfs_context: dict[str, Any],
        device_context: dict[str, Any],
    ) -> dict[str, Any]:
        """
        Assess the risk level for a ZFS operation.

        Args:
            operation_type: Type of ZFS operation
            target_path: ZFS pool/dataset path
            zfs_context: ZFS-specific context information
            device_context: Device and environment context

        Returns:
            Comprehensive risk assessment for the ZFS operation
        """
        logger.info(f"Assessing ZFS operation risk: {operation_type.value} on {target_path}")

        # Classify the target data
        data_classification = self._classify_zfs_data(target_path, zfs_context)

        # Determine protection level
        base_protection = self.operation_base_protection[operation_type]
        data_protection = self.protection_policies[data_classification]
        final_protection = self._merge_protection_levels(base_protection, data_protection)

        # Analyze dependencies and impact
        dependency_analysis = await self._analyze_zfs_dependencies(target_path, zfs_context)
        impact_analysis = await self._analyze_zfs_impact(
            operation_type, target_path, zfs_context, dependency_analysis
        )

        # Check for recent snapshots and backups
        snapshot_analysis = await self._analyze_snapshot_availability(target_path, zfs_context)
        backup_analysis = await self._analyze_backup_status(target_path, zfs_context)

        # Calculate risk score
        risk_score = self._calculate_zfs_risk_score(
            operation_type,
            data_classification,
            dependency_analysis,
            impact_analysis,
            snapshot_analysis,
            backup_analysis,
        )

        # Generate ZFS-specific safety requirements
        safety_requirements = self._generate_zfs_safety_requirements(
            operation_type, final_protection, target_path, zfs_context
        )

        return {
            "operation_type": operation_type.value,
            "target_path": target_path,
            "data_classification": data_classification.value,
            "protection_level": final_protection.value,
            "risk_score": risk_score,
            "risk_level": self._categorize_risk_score(risk_score),
            "dependency_analysis": dependency_analysis,
            "impact_analysis": impact_analysis,
            "snapshot_analysis": snapshot_analysis,
            "backup_analysis": backup_analysis,
            "safety_requirements": safety_requirements,
            "recommended_actions": self._generate_zfs_recommendations(
                operation_type, risk_score, snapshot_analysis, backup_analysis
            ),
        }

    async def validate_zfs_preconditions(
        self,
        operation_type: ZFSOperationType,
        target_path: str,
        zfs_context: dict[str, Any],
        safety_requirements: dict[str, Any],
    ) -> dict[str, Any]:
        """
        Validate that all preconditions are met for a ZFS operation.

        Args:
            operation_type: Type of ZFS operation
            target_path: ZFS pool/dataset path
            zfs_context: ZFS-specific context
            safety_requirements: Safety requirements to validate

        Returns:
            Validation results with pass/fail status
        """
        logger.info(f"Validating ZFS preconditions for {operation_type.value} on {target_path}")

        validation_results = {
            "overall_valid": True,
            "validations": [],
            "warnings": [],
            "blocking_issues": [],
        }

        # Validate snapshot requirements
        if safety_requirements.get("snapshot_required", False):
            snapshot_validation = await self._validate_recent_snapshots(target_path, zfs_context)
            validation_results["validations"].append(snapshot_validation)
            if not snapshot_validation["valid"]:
                validation_results["overall_valid"] = False
                validation_results["blocking_issues"].append(snapshot_validation["message"])

        # Validate backup requirements
        if safety_requirements.get("backup_validation_required", False):
            backup_validation = await self._validate_backup_availability(target_path, zfs_context)
            validation_results["validations"].append(backup_validation)
            if not backup_validation["valid"]:
                validation_results["overall_valid"] = False
                validation_results["blocking_issues"].append(backup_validation["message"])

        # Validate dependency requirements
        if safety_requirements.get("dependency_check_required", False):
            dependency_validation = await self._validate_zfs_dependencies(target_path, zfs_context)
            validation_results["validations"].append(dependency_validation)
            if not dependency_validation["valid"]:
                validation_results["overall_valid"] = False
                validation_results["blocking_issues"].append(dependency_validation["message"])

        # Validate system state requirements
        if safety_requirements.get("system_state_check_required", False):
            system_validation = await self._validate_zfs_system_state(target_path, zfs_context)
            validation_results["validations"].append(system_validation)
            if not system_validation["valid"]:
                validation_results["warnings"].append(system_validation["message"])

        # Validate property-specific requirements
        if operation_type == ZFSOperationType.PROPERTY_RESET:
            property_validation = await self._validate_property_reset(target_path, zfs_context)
            validation_results["validations"].append(property_validation)
            if not property_validation["valid"]:
                validation_results["overall_valid"] = False
                validation_results["blocking_issues"].append(property_validation["message"])

        return validation_results

    async def create_protection_snapshot(
        self, target_path: str, operation_type: ZFSOperationType, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """
        Create a protection snapshot before destructive operation.

        Args:
            target_path: ZFS pool/dataset path
            operation_type: Type of operation requiring protection
            zfs_context: ZFS context information

        Returns:
            Snapshot creation results
        """
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        snapshot_name = f"{target_path}@protection_{operation_type.value}_{timestamp}"

        logger.info(f"Creating protection snapshot: {snapshot_name}")

        # In production, this would execute the actual ZFS snapshot command
        # For now, we simulate the operation

        try:
            snapshot_result = {
                "snapshot_name": snapshot_name,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "operation_type": operation_type.value,
                "target_path": target_path,
                "size_estimate": zfs_context.get("dataset_size", "unknown"),
                "creation_successful": True,
                "snapshot_command": f"zfs snapshot {snapshot_name}",
            }

            # Add recursive flag for datasets with children
            if zfs_context.get("has_children", False):
                snapshot_result["recursive"] = True
                snapshot_result["snapshot_command"] = f"zfs snapshot -r {snapshot_name}"

            logger.info(f"Protection snapshot created successfully: {snapshot_name}")
            return snapshot_result

        except Exception as e:
            logger.error(f"Failed to create protection snapshot: {e}")
            return {
                "snapshot_name": snapshot_name,
                "creation_successful": False,
                "error": str(e),
                "created_at": datetime.now(timezone.utc).isoformat(),
            }

    def _classify_zfs_data(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> ZFSDataClassification:
        """Classify ZFS data based on path and properties."""

        # System critical datasets
        system_patterns = ["/", "rpool", "rpool/ROOT", "rpool/var", "boot"]
        if any(pattern in target_path.lower() for pattern in system_patterns):
            return ZFSDataClassification.SYSTEM_CRITICAL

        # Application data patterns
        app_patterns = ["postgres", "mysql", "mongodb", "database", "db", "app", "application"]
        if any(pattern in target_path.lower() for pattern in app_patterns):
            return ZFSDataClassification.APPLICATION_DATA

        # User data patterns
        user_patterns = ["home", "users", "user", "documents", "personal"]
        if any(pattern in target_path.lower() for pattern in user_patterns):
            return ZFSDataClassification.USER_DATA

        # Backup data patterns
        backup_patterns = ["backup", "backups", "bak", "archive", "snapshot"]
        if any(pattern in target_path.lower() for pattern in backup_patterns):
            return ZFSDataClassification.BACKUP_DATA

        # Temporary data patterns
        temp_patterns = ["tmp", "temp", "cache", "swap", "spool"]
        if any(pattern in target_path.lower() for pattern in temp_patterns):
            return ZFSDataClassification.TEMPORARY

        # Check ZFS properties for additional classification hints
        mountpoint = zfs_context.get("mountpoint", "")
        if mountpoint:
            if any(pattern in mountpoint.lower() for pattern in system_patterns):
                return ZFSDataClassification.SYSTEM_CRITICAL
            elif any(pattern in mountpoint.lower() for pattern in user_patterns):
                return ZFSDataClassification.USER_DATA

        # Default to application data for unclassified datasets
        return ZFSDataClassification.APPLICATION_DATA

    def _merge_protection_levels(
        self, level1: ZFSProtectionLevel, level2: ZFSProtectionLevel
    ) -> ZFSProtectionLevel:
        """Merge two protection levels, choosing the higher one."""
        levels_order = [
            ZFSProtectionLevel.MINIMAL,
            ZFSProtectionLevel.STANDARD,
            ZFSProtectionLevel.ENHANCED,
            ZFSProtectionLevel.MAXIMUM,
        ]

        index1 = levels_order.index(level1)
        index2 = levels_order.index(level2)

        return levels_order[max(index1, index2)]

    async def _analyze_zfs_dependencies(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Analyze ZFS dependencies for the target."""

        # In production, this would query actual ZFS dependencies
        mock_dependencies = {
            "child_datasets": zfs_context.get("children", []),
            "dependent_services": [],
            "mounted_applications": [],
            "active_snapshots": zfs_context.get("snapshots", []),
            "clones": [],
            "shares": zfs_context.get("shares", []),
        }

        # Simulate dependency discovery based on path patterns
        if "postgres" in target_path.lower():
            mock_dependencies["dependent_services"] = ["postgresql", "pgbouncer"]
            mock_dependencies["mounted_applications"] = ["database_app"]
        elif "home" in target_path.lower():
            mock_dependencies["dependent_services"] = ["ssh", "user_sessions"]
            mock_dependencies["mounted_applications"] = ["user_applications"]

        dependency_count = (
            len(mock_dependencies["child_datasets"])
            + len(mock_dependencies["dependent_services"])
            + len(mock_dependencies["mounted_applications"])
        )

        return {
            "dependencies": mock_dependencies,
            "dependency_count": dependency_count,
            "has_dependencies": dependency_count > 0,
            "critical_dependencies": len(mock_dependencies["dependent_services"]) > 0,
        }

    async def _analyze_zfs_impact(
        self,
        operation_type: ZFSOperationType,
        target_path: str,
        zfs_context: dict[str, Any],
        dependency_analysis: dict[str, Any],
    ) -> dict[str, Any]:
        """Analyze the impact of the ZFS operation."""

        impact_factors = {
            "data_loss_risk": "high"
            if operation_type
            in [
                ZFSOperationType.POOL_DESTROY,
                ZFSOperationType.DATASET_DESTROY,
                ZFSOperationType.VOLUME_DESTROY,
            ]
            else "low",
            "service_disruption": "high" if dependency_analysis["critical_dependencies"] else "low",
            "recovery_complexity": "high"
            if operation_type == ZFSOperationType.POOL_DESTROY
            else "medium",
            "estimated_downtime_minutes": self._estimate_zfs_downtime(operation_type, zfs_context),
        }

        # Calculate affected data size
        dataset_size = zfs_context.get("used_space", "0B")
        impact_factors["affected_data_size"] = dataset_size

        # Determine reversibility
        reversible_operations = [
            ZFSOperationType.POOL_EXPORT,
            ZFSOperationType.POOL_OFFLINE,
            ZFSOperationType.DATASET_RENAME,
        ]
        impact_factors["reversible"] = operation_type in reversible_operations

        return impact_factors

    async def _analyze_snapshot_availability(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Analyze available snapshots for the target."""

        snapshots = zfs_context.get("snapshots", [])

        # Simulate snapshot analysis
        recent_snapshots = []
        for i, snapshot in enumerate(snapshots[:5]):  # Check last 5 snapshots
            hours_old = i * 6  # Simulate snapshots every 6 hours
            recent_snapshots.append(
                {
                    "name": snapshot,
                    "age_hours": hours_old,
                    "size": f"{100 + i * 50}MB",
                    "creation_time": (
                        datetime.now(timezone.utc) - timedelta(hours=hours_old)
                    ).isoformat(),
                }
            )

        return {
            "total_snapshots": len(snapshots),
            "recent_snapshots": recent_snapshots,
            "newest_snapshot_age_hours": recent_snapshots[0]["age_hours"]
            if recent_snapshots
            else 999,
            "has_recent_snapshot": len(recent_snapshots) > 0
            and recent_snapshots[0]["age_hours"] < 24,
            "snapshot_frequency": "6_hours" if len(recent_snapshots) > 1 else "unknown",
        }

    async def _analyze_backup_status(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Analyze backup status for the target."""

        # In production, this would check actual backup systems
        mock_backup_status = {
            "has_backups": True,
            "last_backup_age_hours": 12,
            "last_backup_time": (datetime.now(timezone.utc) - timedelta(hours=12)).isoformat(),
            "backup_frequency": "daily",
            "backup_destination": "remote_storage",
            "backup_verification": "verified",
            "estimated_recovery_time_hours": 2,
        }

        # Adjust based on data classification
        if "tmp" in target_path.lower() or "cache" in target_path.lower():
            mock_backup_status["has_backups"] = False
            mock_backup_status["backup_frequency"] = "none"

        return mock_backup_status

    def _calculate_zfs_risk_score(
        self,
        operation_type: ZFSOperationType,
        data_classification: ZFSDataClassification,
        dependency_analysis: dict[str, Any],
        impact_analysis: dict[str, Any],
        snapshot_analysis: dict[str, Any],
        backup_analysis: dict[str, Any],
    ) -> float:
        """Calculate comprehensive risk score for ZFS operation."""

        base_scores = {
            ZFSOperationType.POOL_DESTROY: 10.0,
            ZFSOperationType.DATASET_DESTROY: 8.0,
            ZFSOperationType.VOLUME_DESTROY: 8.0,
            ZFSOperationType.SNAPSHOT_ROLLBACK: 6.0,
            ZFSOperationType.DATASET_RENAME: 4.0,
            ZFSOperationType.SNAPSHOT_DESTROY: 3.0,
            ZFSOperationType.POOL_OFFLINE: 5.0,
            ZFSOperationType.POOL_EXPORT: 3.0,
            ZFSOperationType.PROPERTY_RESET: 2.0,
        }

        classification_multipliers = {
            ZFSDataClassification.SYSTEM_CRITICAL: 1.5,
            ZFSDataClassification.APPLICATION_DATA: 1.2,
            ZFSDataClassification.USER_DATA: 1.1,
            ZFSDataClassification.BACKUP_DATA: 0.8,
            ZFSDataClassification.ARCHIVE: 0.9,
            ZFSDataClassification.TEMPORARY: 0.5,
        }

        risk_score = base_scores[operation_type] * classification_multipliers[data_classification]

        # Adjust for dependencies
        if dependency_analysis["critical_dependencies"]:
            risk_score *= 1.3
        elif dependency_analysis["has_dependencies"]:
            risk_score *= 1.1

        # Adjust for snapshot availability
        if not snapshot_analysis["has_recent_snapshot"]:
            risk_score *= 1.4
        elif snapshot_analysis["newest_snapshot_age_hours"] > 72:
            risk_score *= 1.2

        # Adjust for backup status
        if not backup_analysis["has_backups"]:
            risk_score *= 1.3
        elif backup_analysis["last_backup_age_hours"] > 48:
            risk_score *= 1.1

        # Adjust for reversibility
        if not impact_analysis["reversible"]:
            risk_score *= 1.2

        return min(10.0, max(1.0, risk_score))

    def _categorize_risk_score(self, risk_score: float) -> str:
        """Categorize numeric risk score into risk level."""
        if risk_score >= 8.0:
            return "critical"
        elif risk_score >= 6.0:
            return "high"
        elif risk_score >= 4.0:
            return "medium"
        else:
            return "low"

    def _generate_zfs_safety_requirements(
        self,
        operation_type: ZFSOperationType,
        protection_level: ZFSProtectionLevel,
        target_path: str,
        zfs_context: dict[str, Any],
    ) -> dict[str, Any]:
        """Generate ZFS-specific safety requirements."""

        requirements = {
            "confirmation_required": True,
            "confirmation_phrase_required": protection_level != ZFSProtectionLevel.MINIMAL,
            "snapshot_required": protection_level
            in [ZFSProtectionLevel.ENHANCED, ZFSProtectionLevel.MAXIMUM],
            "backup_validation_required": protection_level == ZFSProtectionLevel.MAXIMUM,
            "dependency_check_required": protection_level
            in [ZFSProtectionLevel.ENHANCED, ZFSProtectionLevel.MAXIMUM],
            "system_state_check_required": protection_level == ZFSProtectionLevel.MAXIMUM,
            "manual_review_required": protection_level == ZFSProtectionLevel.MAXIMUM,
            "multi_admin_approval": protection_level == ZFSProtectionLevel.MAXIMUM
            and operation_type == ZFSOperationType.POOL_DESTROY,
        }

        # Add operation-specific requirements
        if operation_type in [ZFSOperationType.POOL_DESTROY, ZFSOperationType.DATASET_DESTROY]:
            requirements["final_confirmation_required"] = True
            requirements["confirmation_delay_seconds"] = 30

        if operation_type == ZFSOperationType.SNAPSHOT_ROLLBACK:
            requirements["rollback_impact_review"] = True

        return requirements

    def _generate_zfs_recommendations(
        self,
        operation_type: ZFSOperationType,
        risk_score: float,
        snapshot_analysis: dict[str, Any],
        backup_analysis: dict[str, Any],
    ) -> list[str]:
        """Generate ZFS-specific recommendations."""

        recommendations = []

        if risk_score >= 8.0:
            recommendations.append("ðŸ”´ CRITICAL: Consider alternative approaches before proceeding")

        if not snapshot_analysis["has_recent_snapshot"]:
            recommendations.append("ðŸ“¸ Create a snapshot before proceeding")

        if not backup_analysis["has_backups"]:
            recommendations.append("ðŸ’¾ Ensure backups are available before proceeding")
        elif backup_analysis["last_backup_age_hours"] > 24:
            recommendations.append("ðŸ”„ Consider creating a fresh backup")

        if operation_type == ZFSOperationType.POOL_DESTROY:
            recommendations.append(
                "âš ï¸ Pool destruction is irreversible - verify all data is backed up"
            )
            recommendations.append("ðŸ” Double-check the target pool name")

        if operation_type == ZFSOperationType.SNAPSHOT_ROLLBACK:
            recommendations.append("âª Snapshot rollback will lose all changes after the snapshot")

        return recommendations

    def _estimate_zfs_downtime(
        self, operation_type: ZFSOperationType, zfs_context: dict[str, Any]
    ) -> int:
        """Estimate downtime in minutes for ZFS operation."""

        base_times = {
            ZFSOperationType.POOL_DESTROY: 5,
            ZFSOperationType.POOL_EXPORT: 2,
            ZFSOperationType.POOL_OFFLINE: 1,
            ZFSOperationType.DATASET_DESTROY: 3,
            ZFSOperationType.DATASET_RENAME: 1,
            ZFSOperationType.SNAPSHOT_DESTROY: 1,
            ZFSOperationType.SNAPSHOT_ROLLBACK: 5,
            ZFSOperationType.VOLUME_DESTROY: 2,
            ZFSOperationType.PROPERTY_RESET: 1,
        }

        base_time = base_times[operation_type]

        # Adjust for dataset size (simulate)
        dataset_size_gb = zfs_context.get("size_gb", 10)
        if dataset_size_gb > 100:
            base_time *= 2
        elif dataset_size_gb > 1000:
            base_time *= 3

        return base_time

    async def _validate_recent_snapshots(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate that recent snapshots exist."""

        snapshots = zfs_context.get("snapshots", [])

        if not snapshots:
            return {
                "valid": False,
                "message": "No snapshots available - create snapshot before proceeding",
                "validation_type": "snapshot_availability",
            }

        # Check if newest snapshot is recent (within 24 hours)
        # In production, this would check actual snapshot timestamps
        mock_newest_age = 6  # hours

        if mock_newest_age > 24:
            return {
                "valid": False,
                "message": f"Newest snapshot is {mock_newest_age} hours old - create fresh snapshot",
                "validation_type": "snapshot_freshness",
            }

        return {
            "valid": True,
            "message": f"Recent snapshot available ({mock_newest_age} hours old)",
            "validation_type": "snapshot_availability",
        }

    async def _validate_backup_availability(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate that backups are available and recent."""

        # Mock backup validation - in production would check actual backup systems
        has_backup = not any(pattern in target_path.lower() for pattern in ["tmp", "cache", "temp"])

        if not has_backup:
            return {
                "valid": False,
                "message": "No backups available - ensure data is backed up before proceeding",
                "validation_type": "backup_availability",
            }

        return {
            "valid": True,
            "message": "Backup verification completed - recent backups available",
            "validation_type": "backup_availability",
        }

    async def _validate_zfs_dependencies(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate ZFS dependencies and mounted systems."""

        # Mock dependency validation
        has_critical_deps = "postgres" in target_path.lower() or "mysql" in target_path.lower()

        if has_critical_deps:
            return {
                "valid": False,
                "message": "Critical services depend on this dataset - stop services first",
                "validation_type": "dependency_check",
            }

        return {
            "valid": True,
            "message": "No blocking dependencies found",
            "validation_type": "dependency_check",
        }

    async def _validate_zfs_system_state(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate ZFS system state and health."""

        return {
            "valid": True,
            "message": "ZFS system state healthy - safe to proceed",
            "validation_type": "system_state",
        }

    async def _validate_property_reset(
        self, target_path: str, zfs_context: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate property reset operations."""

        property_name = zfs_context.get("property_name", "")

        if property_name in self.critical_properties:
            return {
                "valid": False,
                "message": f"Property '{property_name}' is critical - manual review required",
                "validation_type": "property_validation",
            }

        return {
            "valid": True,
            "message": f"Property '{property_name}' reset validation passed",
            "validation_type": "property_validation",
        }

    def get_supported_operations(self) -> list[dict[str, Any]]:
        """Get list of supported ZFS operations."""
        return [
            {
                "operation": op.value,
                "base_protection": self.operation_base_protection[op].value,
                "description": self._get_operation_description(op),
            }
            for op in ZFSOperationType
        ]

    def _get_operation_description(self, operation: ZFSOperationType) -> str:
        """Get human-readable description for ZFS operation."""
        descriptions = {
            ZFSOperationType.POOL_DESTROY: "Permanently destroy ZFS pool and all data",
            ZFSOperationType.POOL_EXPORT: "Export ZFS pool (reversible)",
            ZFSOperationType.POOL_OFFLINE: "Take ZFS pool offline (reversible)",
            ZFSOperationType.DATASET_DESTROY: "Permanently destroy ZFS dataset",
            ZFSOperationType.DATASET_RENAME: "Rename ZFS dataset (reversible)",
            ZFSOperationType.SNAPSHOT_DESTROY: "Delete ZFS snapshot",
            ZFSOperationType.SNAPSHOT_ROLLBACK: "Rollback to ZFS snapshot (loses recent changes)",
            ZFSOperationType.VOLUME_DESTROY: "Permanently destroy ZFS volume",
            ZFSOperationType.PROPERTY_RESET: "Reset ZFS property to default value",
        }
        return descriptions[operation]

    def get_protection_statistics(self) -> dict[str, Any]:
        """Get ZFS protection service statistics."""
        return {
            "supported_operations": len(ZFSOperationType),
            "protection_levels": len(ZFSProtectionLevel),
            "data_classifications": len(ZFSDataClassification),
            "critical_properties": len(self.critical_properties),
            "protection_features": {
                "automatic_snapshots": True,
                "dependency_analysis": True,
                "backup_validation": True,
                "risk_assessment": True,
                "multi_level_protection": True,
                "property_validation": True,
            },
        }
